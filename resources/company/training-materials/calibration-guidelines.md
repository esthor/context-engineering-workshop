# Performance Review Calibration Guidelines

## Purpose of Calibration

Calibration sessions ensure consistency and fairness in performance ratings across teams. Managers meet to review draft ratings and ensure they're applying the same standards.

## Calibration Principles

### 1. Evidence-Based Decisions
- Ratings must be supported by specific examples
- Impact should be measurable where possible
- Avoid recency bias - consider entire review period

### 2. Relative to Level Expectations
- "Meets Expectations" means doing the job of the level well
- Compare performance to level expectations, not to other individuals
- Use level expectations document as primary reference

### 3. Rating Distribution Guidelines

**Not Quotas, But Expectations:**
- Beyond Expectations: ~15-20% of engineers
  - Significantly exceeds level requirements
  - Demonstrates sustained excellence across multiple dimensions
  - Creates impact beyond core role responsibilities

- Meets Expectations: ~70-80% of engineers
  - Solid, reliable performance at level
  - This is the expected and respected standard
  - Meets all key requirements of the role

- Doesn't Meet Expectations: ~5-10% of engineers
  - Performance gaps in critical areas
  - Requires improvement plan
  - Should not be a surprise to the employee

### 4. Avoid Common Biases

**Recency Bias**: Don't overweight recent events vs. full review period

**Halo Effect**: One outstanding achievement shouldn't inflate all ratings

**Leniency Bias**: Being "nice" by inflating ratings hurts the individual and team

**Comparison Bias**: Compare to level expectations, not to highest performer

**Attribution Bias**: Consider context - was success due to individual or favorable circumstances?

## Calibration Meeting Process

### Preparation (Before Meeting)
- Complete all draft reviews
- Gather supporting evidence
- Be ready to discuss each rating with specifics

### During Calibration
1. Review rating distribution across organization
2. Discuss each "Beyond" and "Doesn't Meet" rating in detail
3. Spot check "Meets" ratings for consistency
4. Challenge ratings that seem inconsistent
5. Adjust ratings based on discussion
6. Ensure development plans for all "Doesn't Meet" ratings

### Discussion Framework

For each review discussed:
- **What**: What did the person accomplish?
- **Impact**: What was the measurable impact?
- **How**: How does this compare to level expectations?
- **Context**: What were the circumstances (team size, support, etc.)?
- **Consistency**: How does this compare to similar ratings?

## Common Calibration Scenarios

### Scenario: High Output, Low Impact
**Question**: Engineer delivers many projects but limited business impact.

**Discussion**: Does quantity compensate for impact? Usually this is "Meets" unless scope/complexity is exceptional.

### Scenario: One Outstanding Achievement
**Question**: Engineer had one major win but otherwise solid performance.

**Discussion**: Was the achievement significantly beyond role expectations? One great project usually doesn't warrant "Beyond" unless it's truly exceptional impact.

### Scenario: Strong Technical Skills, Weak Collaboration
**Question**: Brilliant engineer but difficult to work with.

**Discussion**: Collaboration is a key dimension. Can't be "Beyond" overall with weak collaboration. Usually "Meets" with clear development plan.

### Scenario: New to Level
**Question**: Recently promoted to L5, doing well but not exceeding.

**Discussion**: Should be rated against L5 expectations, even if new. Doing well at new level = "Meets", which is good!

### Scenario: Almost Staff Level
**Question**: L5 doing some Staff-level work but not consistently.

**Discussion**: Could be "Beyond" as L5, but that's different from ready for promotion. Note Staff potential in review.

## Red Flags in Calibration

Watch for:
- Manager can't provide specific examples
- Rating seems based on potential rather than performance
- All ratings are "Meets" (possible leniency bias)
- "Beyond" rating without clear evidence of exceptional impact
- "Doesn't Meet" that would surprise the employee
- Inconsistent ratings across similar roles/accomplishments

## After Calibration

- Update reviews based on calibration discussion
- Ensure all reviews have specific examples
- Prepare for review delivery conversations
- For "Doesn't Meet" ratings, create detailed improvement plans
- For "Beyond" ratings, ensure recognition is meaningful
