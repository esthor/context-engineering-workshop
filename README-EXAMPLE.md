# Context Engineering Workshop - Example System

This is a demonstration of **Context Engineering** principles for building modular LLM applications.

## ğŸ¯ Core Principle

**Minimize context in each LLM invocation.** Instead of dumping all resources into one massive prompt, we use specialized agents that each process only what they need.

## ğŸ“ Project Structure

```
context-engineering-workshop/
â”œâ”€â”€ resources/              # Source data (minimal context per agent)
â”‚   â”œâ”€â”€ company/           # Company-wide materials
â”‚   â”‚   â”œâ”€â”€ training-materials/
â”‚   â”‚   â”‚   â”œâ”€â”€ performance-review-guidelines.md
â”‚   â”‚   â”‚   â””â”€â”€ calibration-guidelines.md
â”‚   â”‚   â””â”€â”€ level-expectations.md
â”‚   â”œâ”€â”€ reviewee/          # Employee-specific information
â”‚   â”‚   â”œâ”€â”€ employee-profile.md
â”‚   â”‚   â””â”€â”€ work-history-h2-2024.md
â”‚   â”œâ”€â”€ manager/           # Manager observations
â”‚   â”‚   â”œâ”€â”€ 1on1-notes-h2-2024.md
â”‚   â”‚   â””â”€â”€ peer-feedback.md
â”‚   â””â”€â”€ schema/            # Output schema
â”‚       â””â”€â”€ schema.md
â”‚
â”œâ”€â”€ prompts/               # Task-specific prompts
â”‚   â”œâ”€â”€ extract-framework.md
â”‚   â”œâ”€â”€ extract-employee-context.md
â”‚   â”œâ”€â”€ extract-manager-insights.md
â”‚   â”œâ”€â”€ synthesize-review.md
â”‚   â””â”€â”€ review-quality-check.md
â”‚
â”œâ”€â”€ agents/                # Specialized agents (each with minimal context)
â”‚   â”œâ”€â”€ framework-agent.py
â”‚   â”œâ”€â”€ employee-agent.py
â”‚   â”œâ”€â”€ manager-agent.py
â”‚   â”œâ”€â”€ synthesis-agent.py
â”‚   â””â”€â”€ reviewer-agent.py
â”‚
â””â”€â”€ orchestrator.py        # Coordinates the agent pipeline
```

## ğŸ”„ Agent Pipeline

### Traditional Approach âŒ
```
All Resources (50K+ tokens) â†’ Single LLM Call â†’ Review
```
**Problems:** Expensive, slow, model may miss details in large context

### Context-Engineered Approach âœ…
```
Company Docs â†’ FrameworkAgent â†’ Framework Summary (500 tokens)
Employee Files â†’ EmployeeAgent â†’ Accomplishments (800 tokens)
Manager Notes â†’ ManagerAgent â†’ Insights (800 tokens)
                        â†“
         [3 Condensed Summaries (2.1K tokens)]
                        â†“
              SynthesisAgent â†’ Draft Review
                        â†“
              ReviewerAgent â†’ Quality Check
```
**Benefits:** Fast, cheap, focused, debuggable

## ğŸ¤– The Agents

### 1. FrameworkAgent
- **Context:** Company training materials only
- **Task:** Extract evaluation dimensions, rating scale, guidelines
- **Output:** Structured framework (~500 tokens)

### 2. EmployeeAgent
- **Context:** Employee profile and work history only
- **Task:** Extract accomplishments, projects, skills, impact
- **Output:** Factual summary of employee's work (~800 tokens)

### 3. ManagerAgent
- **Context:** 1:1 notes and peer feedback only
- **Task:** Extract themes, insights, stories, growth trajectory
- **Output:** Qualitative insights (~800 tokens)

### 4. SynthesisAgent
- **Context:** Condensed outputs from agents 1-3
- **Task:** Write complete performance review following framework
- **Output:** Draft review (~2000 tokens)

### 5. ReviewerAgent
- **Context:** Draft review + framework
- **Task:** Quality check against guidelines
- **Output:** Assessment and recommendations (~1500 tokens)

## ğŸš€ Running the Example

```bash
# View the orchestrator demo
python orchestrator.py

# Run individual agents
python agents/framework-agent.py
python agents/employee-agent.py
python agents/manager-agent.py
```

## ğŸ“Š Context Comparison

| Approach | Context Size | LLM Calls | Cost | Quality |
|----------|-------------|-----------|------|---------|
| Traditional | ~50K tokens | 1 | High | Variable |
| Context-Engineered | ~5.6K total | 5 | Low | Focused |

## ğŸ“ Key Lessons

1. **Single-Purpose Agents**: Each agent has one clear task
2. **Minimal Context**: Only load what's needed for the task
3. **Progressive Synthesis**: Condense outputs before combining
4. **Separation of Concerns**: Resources, prompts, and agents are independent
5. **Debuggability**: Easy to inspect and improve individual components

## ğŸ”‘ First Principles (from README.md)

- **Raw, No Intermediaries**: Direct file access, no databases
- **Single-Purpose**: One agent, one job
- **Prepare Everything First**: Resources ready before execution
- **Test Early and Often**: Verify each component
- **0% Context is Best**: Use only what you absolutely need
- **SHIP!**: Working code beats perfect benchmarks

## ğŸ“ Example Data

The `resources/` directory contains realistic dummy data:
- Performance review guidelines
- Employee profile for "Sarah Chen"
- Work history with specific projects and metrics
- Manager's 1:1 notes over 6 months
- Peer feedback from 5 colleagues
- Level expectations (L5 vs L6)
- Calibration guidelines

## ğŸ› ï¸ Next Steps

To turn this into a real application:

1. **Add LLM Integration**: Replace MockLLMClient with Anthropic/OpenAI
2. **Add Error Handling**: Graceful failures, retries
3. **Add Caching**: Cache agent outputs for efficiency
4. **Add Validation**: Ensure outputs match expected format
5. **Add Logging**: Track token usage and performance
6. **Add Tests**: Unit tests for each agent
7. **Scale**: Handle multiple employees in parallel

## ğŸ’¡ Why This Matters

Context engineering is about **making your luck** by designing systems that:
- Work reliably with LLMs (not against them)
- Are easy to debug and improve
- Scale efficiently
- Produce consistent quality

Instead of hoping a single mega-prompt works, we engineer a pipeline of focused, specialized components.

**3 months of hacking with LLMs can save you 5 minutes of context engineering!** â±ï¸
